
@article{albersWhenPowerAnalyses2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  month = jan,
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  file = {G\:\\Mi unidad\\Zotero\\Journal of Experimental Social Psychology\\Albers,Lakens_2018_When power analyses based on pilot data are biased.pdf;D\:\\Zotero Library\\storage\\CXD272UJ\\S002210311630230X.html},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis},
  language = {en}
}

@article{correllAvoidCohenSmall2020,
  title = {Avoid {{Cohen}}'s `{{Small}}', `{{Medium}}', and `{{Large}}' for {{Power Analysis}}},
  author = {Correll, Joshua and Mellinger, Christopher and McClelland, Gary H. and Judd, Charles M.},
  year = {2020},
  month = mar,
  volume = {24},
  pages = {200--207},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.12.009},
  abstract = {One of the most difficult and important decisions in power analysis involves specifying an effect size. Researchers frequently employ definitions of small, medium, and large that were proposed by Jacob Cohen. These definitions are problematic for two reasons. First, they are arbitrary, based on non-scientific criteria. Second, they are inconsistent, changing dramatically and illogically as a function of the statistical test a researcher plans to use (e.g., t-test versus regression). These problems may be unknown to many researchers, but they have a huge impact on power analyses. Estimates of the required n may be inappropriately doubled or cut in half. For power analyses to have any meaning, these definitions of effect size should be avoided.},
  file = {G\:\\Mi unidad\\Zotero\\Trends in Cognitive Sciences\\Correll et al_2020_Avoid Cohen’s ‘Small’, ‘Medium’, and ‘Large’ for Power Analysis.pdf;D\:\\Zotero Library\\storage\\XAGDR5BI\\S1364661319302979.html},
  journal = {Trends in Cognitive Sciences},
  keywords = {effect size,research design,research methods},
  language = {en},
  number = {3}
}

@article{lakensEquivalenceTestsPractical2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Dani{\"e}l},
  year = {2017},
  month = may,
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA; http://web.archive.org/web/20200414002856/https://journals.sagepub.com/doi/10.1177/1948550617697177}},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based...},
  file = {G\:\\Mi unidad\\Zotero\\Social Psychological and Personality Science\\Lakens_2017_Equivalence Tests.pdf;D\:\\Zotero Library\\storage\\955WMPK5\\1948550617697177.html},
  journal = {Social Psychological and Personality Science},
  language = {en}
}

@article{quintanaStatisticalConsiderationsReporting2017,
  title = {Statistical Considerations for Reporting and Planning Heart Rate Variability Case-Control Studies},
  author = {Quintana, Daniel S.},
  year = {2017},
  volume = {54},
  pages = {344--349},
  issn = {1469-8986},
  doi = {10.1111/psyp.12798},
  abstract = {The calculation of heart rate variability (HRV) is a popular tool used to investigate differences in cardiac autonomic control between population samples. When interpreting effect sizes to quantify the magnitude of group differences, researchers typically use Cohen's guidelines of small (0.2), medium (0.5), and large (0.8) effects. However, these guidelines were originally proposed as a fallback for when the effect size distribution (ESD) was unknown. Despite the availability of effect sizes from hundreds of HRV studies, researchers still largely rely on Cohen's guidelines to interpret effect sizes and to perform power analyses to calculate required sample sizes for future research. This article describes an ESD analysis of 297 HRV effect sizes from between-group/case-control studies, revealing that the 25th, 50th, and 75th effect size percentiles correspond with effect sizes of 0.26, 0.51, and 0.88, respectively. The analyses suggest that Cohen's guidelines may underestimate the magnitude of small and large effect sizes and that HRV studies are generally underpowered. Therefore, to better reflect the observed ESD, effect sizes of 0.25, 0.5, and 0.9 should be interpreted as small, medium, and large effects (after rounding to the closest 0.05). Based on power calculations using the ESD, suggested sample sizes are also provided for planning suitably powered studies that are more likely to replicate. Researchers are encouraged to use the ESD data set or their own collected data sets in tandem with the provided analysis script to perform custom ESD and power analyses relevant to their specific research area.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.12798},
  copyright = {\textcopyright{} 2016 Society for Psychophysiological Research},
  file = {D\:\\Zotero Library\\storage\\JY3KFUNC\\psyp.html},
  journal = {Psychophysiology},
  keywords = {Effect size,Heart rate variability,Sample size,Statistical power},
  language = {en},
  number = {3}
}


